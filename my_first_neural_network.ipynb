{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First some theory\n",
    "\n",
    "First some basic calculus:\n",
    "\n",
    "$$\n",
    "f(X) = A \\cdot X\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(X) = X \\cdot A\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{df}{dX} = A\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dg}{dX} = A^T\n",
    "$$\n",
    "\n",
    "Let's start with a simple NN:\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "$$ Z = W \\cdot X + b$$\n",
    "$$ \\hat{Y} = SoftMax(Z)$$\n",
    "\n",
    "where \n",
    "$$\n",
    "SoftMax(Z)_i = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}}\n",
    "$$\n",
    "\n",
    "**Shapes**\n",
    "\n",
    "- $X \\in \\R^{784}$ is a single digit, i.e. a single instance,\n",
    "- $W \\in \\R^{10 \\times 784}$ is the matrix of weights,\n",
    "- $b \\in \\R^{10}$ is the vector of biases,\n",
    "- $Z \\in \\R^{10}$ is the output of the layer,\n",
    "- $\\hat{Y} \\in \\R^{10}$ is our prediction\n",
    "\n",
    "Now, the true vector of values $Y$ is a one hot encoding vector with $1$ in the right label, and $0$ in the other places:\n",
    "\n",
    "$$ \n",
    "Y_i = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "this means that $X_i$ is actually a \"9\".\n",
    "\n",
    "** Loss function **\n",
    "\n",
    "What loss function is better to use in this scenario? One simple possibility would be to use the $MSE(Y, \\hat{Y})$, but in this scenario we have a better option:\n",
    "\n",
    "$$\n",
    "L(Y, \\hat{Y}) = - \\sum y_i \\text{log}(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "why is this a good choice? Because the vector of labels $Y$ is a one-hot-encoding vector, that means that all the values are $0$ expect one that is $1$. Using MSE is not the best option here, we should instead focus on the only component that is not $0$.\n",
    "\n",
    "In this way, summing all the components of $Y$ the only one that survives is the one with the $1$ and its corresponding value in $\\hat{Y}$.\n",
    "\n",
    "And what about $\\text{log}(\\hat{y}_i)$?\n",
    "\n",
    "Ideally $\\hat{y}_i$ should be close to $1$ (and all the other components equal to $0$), and if $\\hat{y}_i \\approx 1$ then $\\text{log}(\\hat{y}_i) \\approx 0$, if instead $\\hat{y}_i \\approx 0$, then $\\text{log}(\\hat{y}_i) \\approx -\\infty$ and $L(Y, \\hat{Y}) \\approx + \\infty$, and it is what we want for a loss function, close to 0 when the prediction is correct and very big when it is wrong.\n",
    "\n",
    "Another reason why it's a good choice is the derivative that exits from it, i.e.:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "if you want to find out why check this article: https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "\n",
    "but what we really want is to calculate the \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Which is easily calculated noting that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial W} = X^T\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "does the shapes match? Well, $\\frac{\\partial L}{\\partial W}$ must be the same shape of $W$ which is $\\R^{10 \\times 784}$, and what we have is that\n",
    "\n",
    "* $(\\hat{Y}-Y) \\in \\R^{10}$\n",
    "* $X \\in \\R^{784} \\implies W^T \\in \\R^{1 \\times 784}$\n",
    "\n",
    "This imply that\n",
    "\n",
    "* $(\\hat{Y}-Y)\\cdot X^T \\in \\R^{10 \\times 784}$ which is exactly what we want\n",
    "\n",
    "Now, what about $b$?\n",
    "\n",
    "We still have that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "but...\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial b} = I \\in \\R^{1 \\times 1}\n",
    "$$\n",
    "\n",
    "and therefore \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = (\\hat{Y}-Y) \\cdot I \\in \\R^{10 \\times 1}\n",
    "$$\n",
    "\n",
    "So putting all together we have+\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$ Z = W \\cdot X + b $$\n",
    "$$ \\hat{Y} = \\text{SoftMax}(Z) $$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$ dZ = (\\hat{Y}-Y) \\in \\R^{10 \\times 1} $$\n",
    "$$ dW = (\\hat{Y}-Y) \\cdot X^T \\in \\R^{10 \\times 784} $$\n",
    "$$ db = (\\hat{Y}-Y) \\cdot I \\in \\R^{10 \\times 1} $$\n",
    "\n",
    "**Parameters update**\n",
    "\n",
    "$$ W^{(n+1)} = W^{(n)} - \\alpha \\cdot dW^{(n)} $$\n",
    "$$ b^{(n+1)} = b^{(n)} - \\alpha \\cdot db^{(n)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing to $n$ observations\n",
    "\n",
    "The formula provided above are for a single observations, i.e. $X \\in \\R^{784}$ is a single digit, a single image, a single observation. \n",
    "What we will have in reality is all the dataset i.e. $ \\mathbb{X} \\in \\R^{784 \\times n}$ where $n$ is the number of observations in my dataset and similarly I will have $\\mathbb{Y} \\in \\R^{10 \\times n}$. \n",
    "\n",
    "What we will have is:\n",
    "\n",
    "* Each column of $\\mathbb{X}$ is a single observation $X_i \\in \\R^{784}$\n",
    "* Each column of $\\mathbb{Y}$ is a single prediction $Y_i \\in \\R^{10}$\n",
    "\n",
    "What we should do when we have $n$ observations?\n",
    "\n",
    "Well let's think about what could be a good strategy:\n",
    "\n",
    "When we had a single observation, lets call it $X_1$ in the end what we end up was \n",
    "\n",
    "$$\n",
    "dW_1 = (\\hat{Y}_1 - Y_1) \\cdot X_1^T \n",
    "$$\n",
    "\n",
    "which is the \"adjustments\" that we must do on $W$ because of the prediction $\\hat{Y}_1$ from $X_1$. \n",
    "If we had another observation, $X_2$ the adjustment would be\n",
    "\n",
    "$$\n",
    "dW_2 = (\\hat{Y}_2 - Y_2) \\cdot X_2^T \n",
    "$$\n",
    "\n",
    "and what would be a good idea to merge these two \"adjustments\"? A simple idea would be to do the average, i.e. to sum and average the two weights \"adjustments\" $W_1$ and $W_2$:\n",
    "\n",
    "$$\n",
    "dW = \\frac{1}{2} \\cdot (W_1 + W_2)\n",
    "$$\n",
    "\n",
    "and this is exactly what will do considering the generalized version:\n",
    "\n",
    "**Shapes**\n",
    "\n",
    "$$ \\mathbb{X} \\in \\R^{784 \\times n} $$\n",
    "$$ \\hat{\\mathbb{Y}} \\in \\R^{10 \\times n}$$\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$ \\mathbb{Z} = W \\cdot \\mathbb{X} + b $$\n",
    "$$ \\hat{\\mathbb{Y}} = \\text{SoftMax}(\\mathbb{Z}) $$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$ d\\mathbb{Z} = (\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\in \\R^{10 \\times n} $$\n",
    "$$ dW = 1/n \\ (\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\cdot \\mathbb{X}^T \\in \\R^{10 \\times 784} $$\n",
    "\n",
    "which considering that\n",
    "\n",
    "$$\n",
    "(\\hat{\\mathbb{Y}}-\\mathbb{Y}) = [\\hat{Y}_1 - Y_1 | \\hat{Y}_2 - Y_2 | \\dots | \\hat{Y}_n - Y_n]\n",
    "$$\n",
    "$$\n",
    "\\mathbb{X} = [X_1 | X_2 | \\dots | X_n]\n",
    "$$\n",
    "\n",
    "doing \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\cdot (\\hat{\\mathbb{Y}}- \\mathbb{Y}) \\cdot \\mathbb{X}^T = \\frac{1}{n} \\cdot ((\\hat{Y}_1 - Y_1)\\cdot X_1^T + \\dots + (\\hat{Y}_n - Y_n)\\cdot X_n^T) = \\frac{1}{n} \\cdot (dW_1 + \\dots + dW_n)\n",
    "$$\n",
    "which is exactly doing the average of all the \"adjustments\" of all the various predictions $\\hat{Y}_1, \\dots, \\hat{Y}_n $\n",
    "\n",
    "Also for $b$ the parameter update for a single observation is simply $\\hat{Y}-Y$ where in this case is $\\in \\R^{10}$, but what if $(\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\in \\R^{10 \\times n}$? Well in this case the better idea would be to do an average since each column here represents one observation, we averege over all the $m$ observations:\n",
    "\n",
    "$$\n",
    "db = 1/m \\ \\Sigma dZ\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition behind parameters update\n",
    "\n",
    "Focus for a moment on the case where $X \\in \\R^{784}$ is a single observation and therefore $\\hat{Y} \\in \\R^{10}$ is a single prediction, we have that\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial W} = X^T\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "what does tell $\\hat{Y}-Y$? \n",
    "\n",
    "Since \n",
    "\n",
    "$$\n",
    "dZ = \\hat{Y}-Y\n",
    "$$\n",
    "\n",
    "suppose to have \n",
    "\n",
    "\n",
    "$$ \n",
    "Y_i = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$$ \n",
    "\\hat{Y}_i = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then \n",
    "\n",
    "\n",
    "$$ \n",
    "dZ = \\hat{Y}_i - Y_i = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           -1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "what does tell to the model? \n",
    "\n",
    "$$\n",
    "Z = Z - dZ = Z - \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           -1\n",
    "    \\end{bmatrix} = Z + \\begin{bmatrix}\n",
    "           -1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           +1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is basically saying, \"Hey man, increase the value of the last value of $Z$\", and since the $SoftMax$ is a monotone increasing funcion of $Z$, this means \n",
    "* increase the probability that $X$ being predicted as a \"9\"!\n",
    "* decrease the probability that $X$ being predicted as a \"0\"!\n",
    "\n",
    "But since \n",
    "\n",
    "$$\n",
    "Z = W \\cdot X + b\n",
    "$$\n",
    "\n",
    "how we should adjust $W$ in order to improve $L$? Well we just saw how to adjust $Z$ in order to improve $L$, let's see how to adjust $Z$ as a function of $W$ and we will have how to adjust $W$ in order to improve $L$. \n",
    "These thing that I have just written in a terrible english is basically the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "\n",
    "And in formula in the end is equal to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "which, if we call $(\\hat{Y}-Y) = \\delta$ and consider \n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "           x_1 \\\\\n",
    "           x_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{784} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we obtain that \n",
    "\n",
    "$$\n",
    "dW = \\frac{\\partial L}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T = [\\delta \\cdot x_1 | \\dots | \\delta \\cdot x_{784}] = \n",
    "\\begin{bmatrix}\n",
    "           x_1, 0, \\dots, 0, x_{784} \\\\\n",
    "           0, 0, \\dots, 0, 0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0, 0, \\dots, 0, 0 \\\\\n",
    "           -x_1, 0, \\dots, 0, -x_{784} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is basically saying to $W$:\n",
    "\n",
    "$$\n",
    "W = W - dW\n",
    "$$\n",
    "\n",
    "\"*Reduce the first row, which is responsible to the probability of the \"0\", and increase the last row, which is responsible for the probability of the \"9\"*.\"\n",
    "\n",
    "The intuition behind $b$ parameter update instead is quite simple, since $b$ only affect by summing on $W \\cdot X$, you simply add/remove the $\\delta$ of the last prediction: $\\hat{Y} - Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to the first layer\n",
    "\n",
    "Ok, so far we have played a little bit, in the sense that this layer is quite simple, lets complicate things a little bit by adding an initial layer with a well know activation function in NN, i.e. the $ReLu$ function:\n",
    "\n",
    "$$\n",
    "Relu(Z)_i = max(0, z_i)\n",
    "$$\n",
    "\n",
    "We obtain for the **Forward Propagation**:\n",
    "\n",
    "$$\n",
    "Z^1 = W^1 \\cdot X + b^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^1 = ReLu(Z^1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^2 = W^2 \\cdot A^1 + b^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{Y} = A^2 = Softmax(Z^2)\n",
    "$$\n",
    "\n",
    "and since the loss function is always the same, i.e. the **cross entropy loss**:\n",
    "\n",
    "$$\n",
    "L(\\hat{Y}, Y) = -\\sum y_i \\cdot log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "we have for the **Backward Propagation**:\n",
    "\n",
    "$$\n",
    "dZ^2 = (A^2 - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "dW^2 = 1/m \\ dZ^2 \\cdot (A^{1})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^2 = 1/m \\ \\Sigma dZ^2\n",
    "$$\n",
    "\n",
    "Now we need to calculate\n",
    "\n",
    "$$\n",
    "dZ^1 = ... ? \n",
    "$$\n",
    "\n",
    "$$\n",
    "dW^1 = ... ? \n",
    "$$ \n",
    "\n",
    "$$\n",
    "db^1 = ... ?\n",
    "$$\n",
    "\n",
    "Suppose to have $dZ^1$, then the other two would be simply:\n",
    "\n",
    "$$\n",
    "dW^1 = \\frac{d L}{d W^1} = \\frac{d L}{d Z^1} \\cdot \\frac{d Z^1}{ dW^1} = 1/m \\ dZ^1 \\cdot X^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^1 = 1/m \\ \\Sigma dZ^1\n",
    "$$\n",
    "\n",
    "ok now the last part is to compute $dZ^1$\n",
    "\n",
    "$$\n",
    "d Z^1 = \\frac{d L}{d Z^1} = \\frac{d L}{d A^1} \\cdot \\frac{d A^1}{d Z^1}\n",
    "$$\n",
    "\n",
    "where we have that\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d A^1} = g'_{ReLu}(Z^1)\n",
    "$$\n",
    "\n",
    "which we have that is simply the derivative of the $ReLu$ function, i.e.\n",
    "\n",
    "$$\n",
    "g'_{ReLu}(Z^1) = \\begin{cases} \n",
    "    0 & \\text{if } Z^1 < 0 \\\\\n",
    "    1 & \\text{if } Z^1 \\geq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "last we have $\\frac{d L}{d A^1}$ which is simply the derivative of the loss function with respect to the activation of the previous layer, i.e.\n",
    "\n",
    "$$\n",
    "\\frac{d L}{d A^1} = \\frac{d L}{d Z^2} \\cdot \\frac{d Z^2}{d A^1}\n",
    "$$\n",
    "\n",
    "and we have that\n",
    "\n",
    "$$\n",
    "\\frac{d Z^2}{d A^1} = {dW^2}^T\n",
    "$$\n",
    "\n",
    "and finally we have that\n",
    "\n",
    "$$\n",
    "d Z^1 = g'_{ReLu}(Z^1) \\cdot dZ^2 \\cdot {W^2}^T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now (finally) we can implement the whole thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 300\n",
    "g = plt.imshow(X_train[:, idx].reshape(28,28)) \n",
    "print(\"Label: \", Y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "**Data**\n",
    "\n",
    "* $\\mathbb{X} \\in \\R^{m \\times n}$ where $ m = 784$ and $n = 41000$, is our dataset where each column is a digit (aka an observation)\n",
    "* $Y \\in \\R^n$, the labels\n",
    "\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "$$ Z^{[1]} = W^{[1]}\\cdot \\mathbb{X} + b^{[1]}$$\n",
    "$$ A^{[1]} = ReLu(Z^{[1]}) $$\n",
    "$$ Z^{[2]} = W^{[2]}\\cdot A^{[1]} + b^{[2]} $$\n",
    "$$ A^{[2]} = SoftMax(Z^{[2]}) $$\n",
    "\n",
    "**Backward Propagation**\n",
    "\n",
    "$$ dZ^{[2]} = A^{[2]} - Y $$\n",
    "$$ dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T} $$\n",
    "$$ db^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}} $$\n",
    "$$ dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]}) $$\n",
    "$$ dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T} $$\n",
    "$$ db^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}} $$\n",
    "\n",
    "\n",
    "**Parameter updates**\n",
    "\n",
    "$$W^{[2]} := W^{[2]} - \\alpha dW^{[2]}$$\n",
    "$$b^{[2]} := b^{[2]} - \\alpha db^{[2]}$$\n",
    "$$W^{[1]} := W^{[1]} - \\alpha dW^{[1]}$$\n",
    "$$b^{[1]} := b^{[1]} - \\alpha db^{[1]}$$\n",
    "\n",
    "**Vars and shapes**\n",
    "\n",
    "Forward prop\n",
    "\n",
    "- $A^{[0]} = X$: 784 x m\n",
    "- $Z^{[1]} \\sim A^{[1]}$: 10 x m\n",
    "- $W^{[1]}$: 10 x 784 (as $W^{[1]} A^{[0]} \\sim Z^{[1]}$)\n",
    "- $B^{[1]}$: 10 x 1\n",
    "- $Z^{[2]} \\sim A^{[2]}$: 10 x m\n",
    "- $W^{[1]}$: 10 x 10 (as $W^{[2]} A^{[1]} \\sim Z^{[2]}$)\n",
    "- $B^{[2]}$: 10 x 1\n",
    "\n",
    "Backprop\n",
    "\n",
    "- $dZ^{[2]}$: 10 x m ($~A^{[2]}$)\n",
    "- $dW^{[2]}$: 10 x 10\n",
    "- $dB^{[2]}$: 10 x 1\n",
    "- $dZ^{[1]}$: 10 x m ($~A^{[1]}$)\n",
    "- $dW^{[1]}$: 10 x 10\n",
    "- $dB^{[1]}$: 10 x 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def SoftMax(x):\n",
    "    x = np.exp(x) / sum(np.exp(x))\n",
    "    return x\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLu(Z1)\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = SoftMax(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def ReLu_deriv(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def one_hot(Y):\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
    "    one_hot_Y = one_hot(Y)\n",
    "    dZ2 = A2 - one_hot_Y\n",
    "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
    "    db2 = 1 / m * np.sum(dZ2)\n",
    "    dZ1 = W2.T.dot(dZ2) * ReLu_deriv(Z1)\n",
    "    dW1 = 1 / m * dZ1.dot(X.T)\n",
    "    db1 = 1 / m * np.sum(dZ1)\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2    \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, 0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iterations):\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Iteration: \", i)\n",
    "            predictions = get_predictions(A2)\n",
    "            print(\"Accuracy: \", get_accuracy(predictions, Y))\n",
    "            print(\"-\"*20)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Accuracy:  0.15692682926829268\n",
      "--------------------\n",
      "Iteration:  10\n",
      "Accuracy:  0.2906829268292683\n",
      "--------------------\n",
      "Iteration:  20\n",
      "Accuracy:  0.4017804878048781\n",
      "--------------------\n",
      "Iteration:  30\n",
      "Accuracy:  0.45814634146341465\n",
      "--------------------\n",
      "Iteration:  40\n",
      "Accuracy:  0.5018292682926829\n",
      "--------------------\n",
      "Iteration:  50\n",
      "Accuracy:  0.5400243902439025\n",
      "--------------------\n",
      "Iteration:  60\n",
      "Accuracy:  0.5720731707317073\n",
      "--------------------\n",
      "Iteration:  70\n",
      "Accuracy:  0.6002195121951219\n",
      "--------------------\n",
      "Iteration:  80\n",
      "Accuracy:  0.623390243902439\n",
      "--------------------\n",
      "Iteration:  90\n",
      "Accuracy:  0.6419756097560976\n",
      "--------------------\n",
      "Iteration:  100\n",
      "Accuracy:  0.6594878048780488\n",
      "--------------------\n",
      "Iteration:  110\n",
      "Accuracy:  0.673829268292683\n",
      "--------------------\n",
      "Iteration:  120\n",
      "Accuracy:  0.6866585365853659\n",
      "--------------------\n",
      "Iteration:  130\n",
      "Accuracy:  0.6984146341463414\n",
      "--------------------\n",
      "Iteration:  140\n",
      "Accuracy:  0.7089024390243902\n",
      "--------------------\n",
      "Iteration:  150\n",
      "Accuracy:  0.718170731707317\n",
      "--------------------\n",
      "Iteration:  160\n",
      "Accuracy:  0.7265853658536585\n",
      "--------------------\n",
      "Iteration:  170\n",
      "Accuracy:  0.7340975609756097\n",
      "--------------------\n",
      "Iteration:  180\n",
      "Accuracy:  0.7415121951219512\n",
      "--------------------\n",
      "Iteration:  190\n",
      "Accuracy:  0.7486585365853659\n",
      "--------------------\n",
      "Iteration:  200\n",
      "Accuracy:  0.7551951219512195\n",
      "--------------------\n",
      "Iteration:  210\n",
      "Accuracy:  0.7618048780487805\n",
      "--------------------\n",
      "Iteration:  220\n",
      "Accuracy:  0.7672682926829268\n",
      "--------------------\n",
      "Iteration:  230\n",
      "Accuracy:  0.7724878048780488\n",
      "--------------------\n",
      "Iteration:  240\n",
      "Accuracy:  0.7774390243902439\n",
      "--------------------\n",
      "Iteration:  250\n",
      "Accuracy:  0.7820487804878049\n",
      "--------------------\n",
      "Iteration:  260\n",
      "Accuracy:  0.7865853658536586\n",
      "--------------------\n",
      "Iteration:  270\n",
      "Accuracy:  0.7899512195121952\n",
      "--------------------\n",
      "Iteration:  280\n",
      "Accuracy:  0.793609756097561\n",
      "--------------------\n",
      "Iteration:  290\n",
      "Accuracy:  0.7971219512195122\n",
      "--------------------\n",
      "Iteration:  300\n",
      "Accuracy:  0.8003414634146342\n",
      "--------------------\n",
      "Iteration:  310\n",
      "Accuracy:  0.8031951219512196\n",
      "--------------------\n",
      "Iteration:  320\n",
      "Accuracy:  0.8062682926829269\n",
      "--------------------\n",
      "Iteration:  330\n",
      "Accuracy:  0.8090243902439025\n",
      "--------------------\n",
      "Iteration:  340\n",
      "Accuracy:  0.8118536585365854\n",
      "--------------------\n",
      "Iteration:  350\n",
      "Accuracy:  0.8148048780487804\n",
      "--------------------\n",
      "Iteration:  360\n",
      "Accuracy:  0.8175853658536585\n",
      "--------------------\n",
      "Iteration:  370\n",
      "Accuracy:  0.8197560975609756\n",
      "--------------------\n",
      "Iteration:  380\n",
      "Accuracy:  0.8214878048780487\n",
      "--------------------\n",
      "Iteration:  390\n",
      "Accuracy:  0.8241463414634146\n",
      "--------------------\n",
      "Iteration:  400\n",
      "Accuracy:  0.8260731707317073\n",
      "--------------------\n",
      "Iteration:  410\n",
      "Accuracy:  0.8285609756097561\n",
      "--------------------\n",
      "Iteration:  420\n",
      "Accuracy:  0.8304878048780487\n",
      "--------------------\n",
      "Iteration:  430\n",
      "Accuracy:  0.832170731707317\n",
      "--------------------\n",
      "Iteration:  440\n",
      "Accuracy:  0.8336097560975609\n",
      "--------------------\n",
      "Iteration:  450\n",
      "Accuracy:  0.8353414634146341\n",
      "--------------------\n",
      "Iteration:  460\n",
      "Accuracy:  0.8372926829268292\n",
      "--------------------\n",
      "Iteration:  470\n",
      "Accuracy:  0.8385609756097561\n",
      "--------------------\n",
      "Iteration:  480\n",
      "Accuracy:  0.840170731707317\n",
      "--------------------\n",
      "Iteration:  490\n",
      "Accuracy:  0.8414146341463414\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, W1, b1, W2, b2):\n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = get_predictions(A2)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test set:  0.845\n"
     ]
    }
   ],
   "source": [
    "dev_predictions = make_predictions(X_dev, W1, b1, W2, b2)\n",
    "print(\"Accuracy on Test set: \", get_accuracy(dev_predictions, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 ('venv_grokking': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d39242bd6e76d986cf9b4723d85fa1341b3a788ee40f143d2f6848ec53387a4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
