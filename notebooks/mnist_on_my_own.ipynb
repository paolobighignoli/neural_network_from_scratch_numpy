{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First some theory\n",
    "\n",
    "First some basic calculus:\n",
    "\n",
    "$$\n",
    "f(X) = A \\cdot X\n",
    "$$\n",
    "\n",
    "$$\n",
    "g(X) = X \\cdot A\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{df}{dX} = A\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dg}{dX} = A^T\n",
    "$$\n",
    "\n",
    "Let's start with a simple NN:\n",
    "\n",
    "**Forward Propagation**\n",
    "\n",
    "$$ Z = W \\cdot X + b$$\n",
    "$$ \\hat{Y} = SoftMax(Z)$$\n",
    "\n",
    "where \n",
    "$$\n",
    "SoftMax(Z)_i = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}}\n",
    "$$\n",
    "\n",
    "**Shapes**\n",
    "\n",
    "- $X \\in \\R^{784}$ is a single digit, i.e. a single instance,\n",
    "- $W \\in \\R^{10 \\times 784}$ is the matrix of weights,\n",
    "- $b \\in \\R^{10}$ is the vector of biases,\n",
    "- $Z \\in \\R^{10}$ is the output of the layer,\n",
    "- $\\hat{Y} \\in \\R^{10}$ is our prediction\n",
    "\n",
    "Now, the true vector of values $Y$ is a one hot encoding vector with $1$ in the right label, and $0$ in the other places:\n",
    "\n",
    "$$ \n",
    "Y_i = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "this means that $X_i$ is actually a \"9\".\n",
    "\n",
    "** Loss function **\n",
    "\n",
    "What loss function is better to use in this scenario? One simple possibility would be to use the $MSE(Y, \\hat{Y})$, but in this scenario we have a better option:\n",
    "\n",
    "$$\n",
    "L(Y, \\hat{Y}) = - \\sum y_i \\text{log}(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "why is this a good choice? Because the vector of labels $Y$ is a one-hot-encoding vector, that means that all the values are $0$ expecte one that is $1$. Using MSE is not the best option here, we should instead focus on the only component that is not $0$.\n",
    "\n",
    "In this way, summing all the components of $Y$ the only one that survives is the one with the $1$ and its corresponding value in $\\hat{Y}$.\n",
    "\n",
    "And what about $\\text{log}(\\hat{y}_i)$?\n",
    "\n",
    "Ideally $\\hat{y}_i$ should be close to $1$ (and all the other components equal to $0$), and if $\\hat{y}_i \\approx 1$ then $\\text{log}(\\hat{y}_i) \\approx 0$, if instead $\\hat{y}_i \\approx 0$, then $\\text{log}(\\hat{y}_i) \\approx -\\infty$ and $L(Y, \\hat{Y}) \\approx + \\infty$, and it is what we want for a loss function, close to 0 when the prediction is correct and very big when it is wrong.\n",
    "\n",
    "Another reason why it's a good choice is the derivative that exits from it, i.e.:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "if you want to find out why check this article: https://levelup.gitconnected.com/killer-combo-softmax-and-cross-entropy-5907442f60ba\n",
    "\n",
    "but what we really want is to calculate the \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "\n",
    "Which is easily calculated noting that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial W} = X^T\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "does the shapes match? Well, $\\frac{\\partial L}{\\partial W}$ must be the same shape of $W$ which is $\\R^{10 \\times 784}$, and what we have is that\n",
    "\n",
    "* $(\\hat{Y}-Y) \\in \\R^{10}$\n",
    "* $X \\in \\R^{784} \\implies W^T \\in \\R^{1 \\times 784}$\n",
    "\n",
    "This imply that\n",
    "\n",
    "* $(\\hat{Y}-Y)\\cdot X^T \\in \\R^{10 \\times 784}$ which is exactly what we want\n",
    "\n",
    "Now, what about $b$?\n",
    "\n",
    "We still have that \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "but...\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial b} = I \\in \\R^{1 \\times 1}\n",
    "$$\n",
    "\n",
    "and therefore \n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = (\\hat{Y}-Y) \\cdot I \\in \\R^{10 \\times 1}\n",
    "$$\n",
    "\n",
    "So putting all together we have+\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$ Z = W \\cdot X + b $$\n",
    "$$ \\hat{Y} = \\text{SoftMax}(Z) $$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$ dZ = (\\hat{Y}-Y) \\in \\R^{10 \\times 1} $$\n",
    "$$ dW = (\\hat{Y}-Y) \\cdot X^T \\in \\R^{10 \\times 784} $$\n",
    "$$ db = (\\hat{Y}-Y) \\cdot I \\in \\R^{10 \\times 1} $$\n",
    "\n",
    "**Parameters update**\n",
    "\n",
    "$$ W = W - \\alpha \\cdot dW $$\n",
    "$$ b = b - \\alpha \\cdot db $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing to $n$ observations\n",
    "\n",
    "The formula provided above are for a single observations, i.e. $X \\in \\R^{784}$ is a single digit, a single image, a single observation. \n",
    "What we will have in reality is all the dataset i.e. $ \\mathbb{X} \\in \\R^{784 \\times n}$ where $n$ is the number of observations in my dataset and similarly I will have $\\mathbb{Y} \\in \\R^{10 \\times n}$. \n",
    "\n",
    "What we will have is:\n",
    "\n",
    "* Each column of $\\mathbb{X}$ is a single observation $X_i \\in \\R^{784}$\n",
    "* Each column of $\\mathbb{Y}$ is a single prediction $Y_i \\in \\R^{10}$\n",
    "\n",
    "What we should do when we have $n$ observations?\n",
    "\n",
    "Well let's think about what could be a good strategy:\n",
    "\n",
    "When we had a single observation, lets call it $X_1$ in the end what we end up was \n",
    "\n",
    "$$\n",
    "dW_1 = (\\hat{Y}_1 - Y_1) \\cdot X_1^T \n",
    "$$\n",
    "\n",
    "which is the \"adjustments\" that we must do on $W$ because of the prediction $\\hat{Y}_1$ from $X_1$. \n",
    "If we had another observation, $X_2$ the adjustment would be\n",
    "\n",
    "$$\n",
    "dW_2 = (\\hat{Y}_2 - Y_2) \\cdot X_2^T \n",
    "$$\n",
    "\n",
    "and what would be a good idea to merge these two \"adjustments\"? A simple idea would be to do the average, i.e. to sum and average the two weights \"adjustments\" $W_1$ and $W_2$:\n",
    "\n",
    "$$\n",
    "dW = \\frac{1}{2} \\cdot (W_1 + W_2)\n",
    "$$\n",
    "\n",
    "and this is exactly what will do considering the generalized version:\n",
    "\n",
    "**Shapes**\n",
    "\n",
    "$$ \\mathbb{X} \\in \\R^{784 \\times n} $$\n",
    "$$ \\hat{\\mathbb{Y}} \\in \\R^{10 \\times n}$$\n",
    "\n",
    "**Forward propagation**\n",
    "\n",
    "$$ \\mathbb{Z} = W \\cdot \\mathbb{X} + b $$\n",
    "$$ \\hat{\\mathbb{Y}} = \\text{SoftMax}(\\mathbb{Z}) $$\n",
    "\n",
    "**Backward propagation**\n",
    "\n",
    "$$ d\\mathbb{Z} = (\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\in \\R^{10 \\times n} $$\n",
    "$$ dW = 1/n \\ (\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\cdot \\mathbb{X}^T \\in \\R^{10 \\times 784} $$\n",
    "\n",
    "which considering that\n",
    "\n",
    "$$\n",
    "(\\hat{\\mathbb{Y}}-\\mathbb{Y}) = [\\hat{Y}_1 - Y_1 | \\hat{Y}_2 - Y_2 | \\dots | \\hat{Y}_n - Y_n]\n",
    "$$\n",
    "$$\n",
    "\\mathbb{X} = [X_1 | X_2 | \\dots | X_n]\n",
    "$$\n",
    "\n",
    "doing \n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\cdot (\\hat{\\mathbb{Y}}- \\mathbb{Y}) \\cdot \\mathbb{X}^T = \\frac{1}{n} \\cdot ((\\hat{Y}_1 - Y_1)\\cdot X_1^T + \\dots + (\\hat{Y}_n - Y_n)\\cdot X_n^T) = \\frac{1}{n} \\cdot (dW_1 + \\dots + dW_n)\n",
    "$$\n",
    "which is exactly doing the average of all the \"adjustments\" of all the various predictions $\\hat{Y}_1, \\dots, \\hat{Y}_n $\n",
    "\n",
    "Also for $b$ the parameter update for a single observation is simply $\\hat{Y}-Y$ where in this case is $\\in \\R^{10}$, but what if $(\\hat{\\mathbb{Y}}-\\mathbb{Y}) \\in \\R^{10 \\times n}$? Well in this case the better idea would be to do an average since each column here represents one observation, we averege over all the $m$ observations:\n",
    "\n",
    "$$\n",
    "db = 1/m \\ \\Sigma dZ\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition behind parameters update\n",
    "\n",
    "Focus for a moment on the case where $X \\in \\R^{784}$ is a single observation and therefore $\\hat{Y} \\in \\R^{10}$ is a single prediction, we have that\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} = \\hat{Y}-Y\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial Z}{\\partial W} = X^T\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "what does tell $\\hat{Y}-Y$? \n",
    "\n",
    "Since \n",
    "\n",
    "$$\n",
    "dZ = \\hat{Y}-Y\n",
    "$$\n",
    "\n",
    "suppose to have \n",
    "\n",
    "\n",
    "$$ \n",
    "Y_i = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "\n",
    "$$ \n",
    "\\hat{Y}_i = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then \n",
    "\n",
    "\n",
    "$$ \n",
    "dZ = \\hat{Y}_i - Y_i = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           -1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "what does tell to the model? \n",
    "\n",
    "$$\n",
    "Z = Z - dZ = Z - \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           -1\n",
    "    \\end{bmatrix} = Z + \\begin{bmatrix}\n",
    "           -1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           +1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is basically saying, \"Hey man, increase the value of the last value of $Z$\", and since the $SoftMax$ is a monotone increasing funcion of $Z$, this means \n",
    "* increase the probability that $X$ being predicted as a \"9\"!\n",
    "* decrease the probability that $X$ being predicted as a \"0\"!\n",
    "\n",
    "But since \n",
    "\n",
    "$$\n",
    "Z = W \\cdot X + b\n",
    "$$\n",
    "\n",
    "how we should adjust $W$ in order to improve $L$? Well we just saw how to adjust $Z$ in order to improve $L$, let's see how to adjust $Z$ as a function of $W$ and we will have how to adjust $W$ in order to improve $L$. \n",
    "These thing that I have just written in a terrible english is basically the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W}\n",
    "$$\n",
    "\n",
    "And in formula in the end is equal to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T\n",
    "$$\n",
    "\n",
    "which, if we call $(\\hat{Y}-Y) = \\delta$ and consider \n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "           x_1 \\\\\n",
    "           x_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{784} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we obtain that \n",
    "\n",
    "$$\n",
    "dW = \\frac{\\partial L}{\\partial W} = (\\hat{Y}-Y) \\cdot X^T = [\\delta \\cdot x_1 | \\dots | \\delta \\cdot x_{784}] = \n",
    "\\begin{bmatrix}\n",
    "           x_1, 0, \\dots, 0, x_{784} \\\\\n",
    "           0, 0, \\dots, 0, 0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0, 0, \\dots, 0, 0 \\\\\n",
    "           -x_1, 0, \\dots, 0, -x_{784} \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which is basically saying to $W$:\n",
    "\n",
    "$$\n",
    "W = W - dW\n",
    "$$\n",
    "\n",
    "\"*Reduce the first row, which is responsible to the probability of the \"0\", and increase the last row, which is responsible for the probability of the \"9\"*.\"\n",
    "\n",
    "The intuition behind $b$ parameter update instead is quite simple, since $b$ only affect by summing on $W \\cdot X$, you simply add/remove the $\\delta$ of the last prediction: $\\hat{Y} - Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move on to the first layer\n",
    "\n",
    "Ok, so far we have played a little bit, in the sense that this layer is quite simple, lets complicate things a little bit by adding an initial layer with a well know activation function in NN, i.e. the $ReLu$ function:\n",
    "\n",
    "$$\n",
    "Relu(Z)_i = max(0, z_i)\n",
    "$$\n",
    "\n",
    "We obtain for the **Forward Propagation**:\n",
    "\n",
    "$$\n",
    "Z^1 = W^1 \\cdot X + b^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^1 = ReLu(Z^1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^2 = W^2 \\cdot A^1 + b^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{Y} = A^2 = Softmax(Z^2)\n",
    "$$\n",
    "\n",
    "and since the loss function is always the same, i.e. the **cross entropy loss**:\n",
    "\n",
    "$$\n",
    "L(\\hat{Y}, Y) = -\\sum y_i \\cdot log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "we have for the **Backward Propagation**:\n",
    "\n",
    "$$\n",
    "dZ^2 = (A^2 - Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "dW^2 = 1/m \\ dZ^2 \\cdot (A^{1})^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^2 = 1/m \\ \\Sigma dZ^2\n",
    "$$\n",
    "\n",
    "Now we need to calculate\n",
    "\n",
    "$$\n",
    "dZ^1 = ... ? \n",
    "$$\n",
    "\n",
    "$$\n",
    "dW^1 = ... ? \n",
    "$$ \n",
    "\n",
    "$$\n",
    "db^1 = ... ?\n",
    "$$\n",
    "\n",
    "Suppose to have $dZ^1$, then the other two would be simply:\n",
    "\n",
    "$$\n",
    "dW^1 = 1/m \\ dZ^1 \\cdot X^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "db^1 = 1/m \\ \\Sigma dZ^1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit ('3.11.1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36aa87528da973003848376161a4d73716901319591f0c4bb70f237c64eec3af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
